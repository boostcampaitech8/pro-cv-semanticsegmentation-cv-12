{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "from custom_dataset import IND2CLASS\n",
    "from SAM2UNet import SAM2UNet\n",
    "\n",
    "# --- [RLE Ïù∏ÏΩîÎî© Ìï®Ïàò] ---\n",
    "def encode_mask_to_rle(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# --- [Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§] ---\n",
    "class XRayInferenceDataset(Dataset):\n",
    "    def __init__(self, image_root, transforms=None):\n",
    "        self.image_root = image_root\n",
    "        \n",
    "        # 1. ÌååÏùº Î¶¨Ïä§Ìä∏ Ï∂îÏ∂ú\n",
    "        pngs = {\n",
    "            os.path.relpath(os.path.join(root, fname), start=image_root)\n",
    "            for root, _dirs, files in os.walk(image_root)\n",
    "            for fname in files\n",
    "            if os.path.splitext(fname)[1].lower() == \".png\"\n",
    "        }\n",
    "        self.filenames = np.array(sorted(list(pngs)))\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # 2. Hand Side (Left/Right) ÌåêÎ≥Ñ Î°úÏßÅ\n",
    "        # Í∞ôÏùÄ Ìè¥Îçî ÎÇ¥ ÌååÏùºÎ™Ö Ï†ïÎ†¨ Ïãú: Ï≤´ Î≤àÏß∏=Right, Îëê Î≤àÏß∏=Left Í∞ÄÏ†ï\n",
    "        self.hand_side_map = {}\n",
    "        files_by_folder = defaultdict(list)\n",
    "        for fname in self.filenames:\n",
    "            folder = os.path.dirname(fname)\n",
    "            files_by_folder[folder].append(fname)\n",
    "            \n",
    "        for folder, files in files_by_folder.items():\n",
    "            files.sort()\n",
    "            if len(files) > 0: self.hand_side_map[files[0]] = 'Right'\n",
    "            if len(files) > 1: self.hand_side_map[files[1]] = 'Left'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image_name = self.filenames[item]\n",
    "        image_path = os.path.join(self.image_root, image_name)\n",
    "        \n",
    "        # 1. Ïù¥ÎØ∏ÏßÄ Î°úÎìú (Gray)\n",
    "        image_gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # 2. [Ï†ÑÏ≤òÎ¶¨] Right -> LeftÎ°ú Ï¢åÏö∞ Î∞òÏ†Ñ (Î™®Îç∏ÏùÄ LeftÎßå ÌïôÏäµÌñàÏúºÎØÄÎ°ú)\n",
    "        hand_side = self.hand_side_map.get(image_name, 'Unknown')\n",
    "        if hand_side == 'Right':\n",
    "            image_gray = cv2.flip(image_gray, 1)\n",
    "        \n",
    "        # 3. 3Ï±ÑÎÑê Î≥ÄÌôò Î∞è Ï†ïÍ∑úÌôî\n",
    "        image = np.stack([image_gray]*3, axis=-1)\n",
    "        image = image / 255.0\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)[\"image\"]\n",
    "\n",
    "        # 4. To Tensor (H, W, C) -> (C, H, W)\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = image.transpose(2, 0, 1)\n",
    "            image = torch.from_numpy(image).float()\n",
    "            \n",
    "        return image, image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e213bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_ROOT = \"../data/test/DCM\"\n",
    "CHECKPOINT_PATH = \"../sam2_unet_result_checkpoints/experiment20.pth\"\n",
    "HIERA_PATH = \"../checkpoints/sam2_hiera_large.pt\"\n",
    "SAVE_PATH = \"result_full_size.csv\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69986a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_full_size(model, data_loader, thr=0.5):\n",
    "    model.eval()\n",
    "    rles = []\n",
    "    filename_and_class = []\n",
    "    dataset = data_loader.dataset\n",
    "    \n",
    "    print(\"üöÄ [Full Size] Inference Start...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, image_names in tqdm(data_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            image_name = image_names[0]\n",
    "            \n",
    "            # 1. Ï∂îÎ°†\n",
    "            preds = model(images)\n",
    "            if isinstance(preds, tuple): preds = preds[0]\n",
    "            \n",
    "            # 2. ÌÅ¨Í∏∞ Î≥¥Ï†ï (ÌòπÏãú 2048Ïù¥ ÏïÑÎãê Í≤ΩÏö∞)\n",
    "            if preds.shape[-1] != 2048:\n",
    "                preds = F.interpolate(preds, size=(2048, 2048), mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            # 3. [Re-flip] Right ÏÜêÏù∏ Í≤ΩÏö∞, Îã§Ïãú ÏõêÎûòÎåÄÎ°ú Î∞òÏ†Ñ\n",
    "            hand_side = dataset.hand_side_map.get(image_name, 'Unknown')\n",
    "            if hand_side == 'Right':\n",
    "                preds = torch.flip(preds, dims=[-1])\n",
    "            \n",
    "            # 4. Thresholding\n",
    "            preds = torch.sigmoid(preds)\n",
    "            preds = (preds > thr).detach().cpu().numpy()\n",
    "            \n",
    "            # 5. RLE Encoding\n",
    "            for output, img_name in zip(preds, image_names):\n",
    "                for c, segm in enumerate(output):\n",
    "                    rle = encode_mask_to_rle(segm)\n",
    "                    rles.append(rle)\n",
    "                    filename_and_class.append(f\"{IND2CLASS[c]}_{img_name}\")\n",
    "                    \n",
    "    return rles, filename_and_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82462ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1024\n",
    "STRIDE = 512\n",
    "\n",
    "def sliding_window_inference(model, image, window_size, stride, num_classes=29):\n",
    "    model.eval()\n",
    "    _, _, H, W = image.shape\n",
    "    prob_map = torch.zeros((1, num_classes, H, W), device=DEVICE)\n",
    "    count_map = torch.zeros((1, num_classes, H, W), device=DEVICE)\n",
    "    \n",
    "    # Ïä§ÌÖù ÏÑ§Ï†ï (ÎßàÏßÄÎßâ ÏûêÌà¨Î¶¨ ÏòÅÏó≠ Ìè¨Ìï®)\n",
    "    h_steps = list(range(0, H - window_size + 1, stride))\n",
    "    w_steps = list(range(0, W - window_size + 1, stride))\n",
    "    if (H - window_size) % stride != 0: h_steps.append(H - window_size)\n",
    "    if (W - window_size) % stride != 0: w_steps.append(W - window_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h in h_steps:\n",
    "            for w in w_steps:\n",
    "                img_patch = image[:, :, h:h+window_size, w:w+window_size]\n",
    "                preds = model(img_patch)\n",
    "                if isinstance(preds, tuple): preds = preds[0]\n",
    "                \n",
    "                prob_patch = torch.sigmoid(preds)\n",
    "                prob_map[:, :, h:h+window_size, w:w+window_size] += prob_patch\n",
    "                count_map[:, :, h:h+window_size, w:w+window_size] += 1.0\n",
    "                \n",
    "    return (prob_map / count_map).squeeze(0) # (C, H, W)\n",
    "\n",
    "def test_sliding(model, data_loader, thr=0.5):\n",
    "    rles = []\n",
    "    filename_and_class = []\n",
    "    dataset = data_loader.dataset\n",
    "    \n",
    "    print(\"üöÄ [Sliding Window] Inference Start...\")\n",
    "    \n",
    "    for images, image_names in tqdm(data_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        image_name = image_names[0]\n",
    "        \n",
    "        # 1. Sliding Window Ï∂îÎ°†\n",
    "        probs = sliding_window_inference(model, images, WINDOW_SIZE, STRIDE)\n",
    "        \n",
    "        # 2. [Re-flip] Right ÏÜêÏù∏ Í≤ΩÏö∞ Î≥µÍµ¨\n",
    "        hand_side = dataset.hand_side_map.get(image_name, 'Unknown')\n",
    "        if hand_side == 'Right':\n",
    "            probs = torch.flip(probs, dims=[-1])\n",
    "            \n",
    "        # 3. Thresholding & RLE\n",
    "        preds = (probs > thr).detach().cpu().numpy()\n",
    "        \n",
    "        for c, segm in enumerate(preds):\n",
    "            rle = encode_mask_to_rle(segm)\n",
    "            rles.append(rle)\n",
    "            filename_and_class.append(f\"{IND2CLASS[c]}_{image_name}\")\n",
    "            \n",
    "    return rles, filename_and_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"sam_sliding\",\n",
    "        \"type\": \"sam2unet\",\n",
    "        \"path\": \"../sam2_unet_result_checkpoints/experiment16.pth\",\n",
    "        \"method\": \"sliding\",\n",
    "        \"weight\": 0.5 \n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sam_full\",\n",
    "        \"type\": \"sam2unet\",\n",
    "        \"path\": \"../sam2_unet_result_checkpoints/experiment20.pth\",\n",
    "        \"method\": \"full\",\n",
    "        \"weight\": 0.5\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- [Ï∂îÎ°† Ìó¨Ìçº Ìï®Ïàò] ---\n",
    "def sliding_window_inference(model, image, window_size=1024, stride=512, num_classes=29):\n",
    "    # (ÏúÑÏùò Ïä¨ÎùºÏù¥Îî© ÏúàÎèÑÏö∞ Ìï®ÏàòÏôÄ ÎèôÏùº)\n",
    "    model.eval()\n",
    "    _, _, H, W = image.shape\n",
    "    prob_map = torch.zeros((1, num_classes, H, W), device=DEVICE)\n",
    "    count_map = torch.zeros((1, num_classes, H, W), device=DEVICE)\n",
    "    \n",
    "    h_steps = list(range(0, H - window_size + 1, stride))\n",
    "    w_steps = list(range(0, W - window_size + 1, stride))\n",
    "    if (H - window_size) % stride != 0: h_steps.append(H - window_size)\n",
    "    if (W - window_size) % stride != 0: w_steps.append(W - window_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for h in h_steps:\n",
    "            for w in w_steps:\n",
    "                img_patch = image[:, :, h:h+window_size, w:w+window_size]\n",
    "                preds = model(img_patch)\n",
    "                if isinstance(preds, tuple): preds = preds[0]\n",
    "                prob_map[:, :, h:h+window_size, w:w+window_size] += torch.sigmoid(preds)\n",
    "                count_map[:, :, h:h+window_size, w:w+window_size] += 1.0\n",
    "                \n",
    "    return (prob_map / count_map).squeeze(0)\n",
    "\n",
    "def ensemble_test(models_list, data_loader, thr=0.5):\n",
    "    rles = []\n",
    "    filename_and_class = []\n",
    "    dataset = data_loader.dataset\n",
    "    \n",
    "    print(\"üöÄ [Soft Voting] Ensemble Start...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, image_names in tqdm(data_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            image_name = image_names[0]\n",
    "            \n",
    "            final_prob = torch.zeros((29, 2048, 2048), device=DEVICE)\n",
    "            total_weight = 0.0\n",
    "            \n",
    "            # --- Í∞Å Î™®Îç∏Î≥Ñ Ï∂îÎ°† Î∞è Í∞ÄÏ§ë Ìï©ÏÇ∞ ---\n",
    "            for config in models_list:\n",
    "                model = config['model']\n",
    "                weight = config['weight']\n",
    "                method = config['method']\n",
    "                \n",
    "                if weight <= 0: continue\n",
    "                \n",
    "                if method == 'sliding':\n",
    "                    prob = sliding_window_inference(model, images)\n",
    "                else: # method == 'full'\n",
    "                    preds = model(images)\n",
    "                    if isinstance(preds, tuple): preds = preds[0]\n",
    "                    # ÌÅ¨Í∏∞ Î≥¥Ï†ï\n",
    "                    if preds.shape[-1] != 2048:\n",
    "                        preds = F.interpolate(preds, size=(2048, 2048), mode=\"bilinear\", align_corners=False)\n",
    "                    prob = torch.sigmoid(preds).squeeze(0)\n",
    "                \n",
    "                final_prob += prob * weight\n",
    "                total_weight += weight\n",
    "            \n",
    "            # Ï†ïÍ∑úÌôî (Weights Ìï©Ïù¥ 1Ïù¥ ÏïÑÎãê Í≤ΩÏö∞ ÎåÄÎπÑ)\n",
    "            if total_weight > 0:\n",
    "                final_prob /= total_weight\n",
    "                \n",
    "            # --- [Re-flip] Right ÏÜêÏù∏ Í≤ΩÏö∞ Î≥µÍµ¨ ---\n",
    "            hand_side = dataset.hand_side_map.get(image_name, 'Unknown')\n",
    "            if hand_side == 'Right':\n",
    "                final_prob = torch.flip(final_prob, dims=[-1])\n",
    "            \n",
    "            # --- Thresholding ---\n",
    "            preds = (final_prob > thr).detach().cpu().numpy()\n",
    "            \n",
    "            for c, segm in enumerate(preds):\n",
    "                rle = encode_mask_to_rle(segm)\n",
    "                rles.append(rle)\n",
    "                filename_and_class.append(f\"{IND2CLASS[c]}_{image_name}\")\n",
    "                \n",
    "    return rles, filename_and_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1. Î™®Îç∏ Ï¥àÍ∏∞Ìôî Î∞è Î°úÎìú\n",
    "    for config in MODEL_CONFIGS:\n",
    "        print(f\"Loading {config['name']}...\")\n",
    "        if config['type'] == 'sam2unet':\n",
    "            model = SAM2UNet(HIERA_PATH).to(DEVICE)\n",
    "        elif config['type'] == 'smp_unetpp':\n",
    "            model = smp.UnetPlusPlus(encoder_name=\"efficientnet-b3\", classes=29).to(DEVICE)\n",
    "        \n",
    "        # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú (state_dict Ï≤òÎ¶¨)\n",
    "        if os.path.exists(config['path']):\n",
    "            ckpt = torch.load(config['path'], map_location=DEVICE)\n",
    "            if isinstance(ckpt, dict) and ('model' in ckpt or 'state_dict' in ckpt):\n",
    "                state_dict = ckpt.get('model', ckpt.get('state_dict'))\n",
    "            elif hasattr(ckpt, 'state_dict'): # Î™®Îç∏ Í∞ùÏ≤¥ ÌÜµÏß∏Î°ú Ï†ÄÏû•Îêú Í≤ΩÏö∞\n",
    "                 state_dict = ckpt.state_dict()\n",
    "            else:\n",
    "                state_dict = ckpt\n",
    "            \n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            config['model'] = model # Î°úÎìúÎêú Î™®Îç∏ Í∞ùÏ≤¥ Ï†ÄÏû•\n",
    "        else:\n",
    "            print(f\"‚ùå Path Not Found: {config['path']}\")\n",
    "            config['weight'] = 0 # Î°úÎìú Ïã§Ìå®Ïãú Í∞ÄÏ§ëÏπò 0 Ï≤òÎ¶¨\n",
    "\n",
    "    # 2. Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    tf = A.Compose([ToTensorV2()])\n",
    "    test_ds = XRayInferenceDataset(IMAGE_ROOT, transforms=tf)\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 3. Ïã§Ìñâ\n",
    "    rles, fnames = ensemble_test(MODEL_CONFIGS, test_loader)\n",
    "    \n",
    "    classes, filename = zip(*[x.split(\"_\") for x in fnames])\n",
    "    df = pd.DataFrame({\"image_name\": [os.path.basename(f) for f in filename], \"class\": classes, \"rle\": rles})\n",
    "    df.to_csv(SAVE_PATH, index=False)\n",
    "    print(f\"‚úÖ Ensemble Result Saved to {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
